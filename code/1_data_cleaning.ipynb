{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from spacy.tokenizer import Tokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Cleaning\n",
    "\n",
    "### Project Contents\n",
    "\n",
    "1. [Data Cleaning](../code/1_data_cleaning.ipynb)\n",
    "2. [Data Visualization](../code/2_eda.ipynb)[This Notebook]\n",
    "3. [Classifcation Model Comparison](../code/3_modeling.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Purpose\n",
    "\n",
    "This notebook is intended to process the \"dirty work\" of data science. We will parse and clean the text information from the scraped r/WallStreetBets and r/Investing subreddits--removing, modifying, and imputing features in a way that will provide for the best classifcation model. \n",
    "\n",
    "---\n",
    "\n",
    "### Note on Data Collection\n",
    "\n",
    "While our original intent was to scrape data from Reddit, recent changes and instability with the Pushshift API made this difficult. Given the time constraints for this deliverable, we chose to use provided data and thus there is no web scraping function or data provided in this notebook. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text to Useful Text\n",
    "\n",
    "With the raw text retrieved from reddit, we need to further process this data before feeding it into our eventual classification model. The raw text has many attributes that are undesirable from a machine learning perspective, and in order to process this text into useful features, we will need to perform a few operations: \n",
    "\n",
    "1. Lower case case all words. \n",
    "\n",
    "2. Remove punctuation/line breaks etc. \n",
    "\n",
    "3. Impute or remove missing/deleted posts. \n",
    "\n",
    "4. Remove self-referential terms. \n",
    "\n",
    "5. Handle hyperlinks/emoji's/other non-traditional text items. \n",
    "\n",
    "6. Remove duplicated posts. \n",
    "\n",
    "7. Stem or lemmatize the text. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in r/WallStreetBets and r/Investing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_df = pd.read_csv('../data/investing.csv')\n",
    "\n",
    "wsb_df = pd.read_csv('../data/wallstreetbets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10033, 4), (10052, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_df.shape, wsb_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by evaluating the balance of classes between the two datasets. If there is an imbalance in the classes, we have one dataset that is over represented and another that is underrepresented. This allows the possibility of bias towards the majority class to creep into our model, and we run the risk of developing a model that will not accurately predict the minority class.\n",
    "\n",
    "We see above that our datasets are quite balanced, which is a good thing. As we further clean the dataset we will continuously check this balance of classes and see what modifications if any are required. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all text\n",
    "\n",
    "inv_df['title'] = inv_df['title'].str.lower()\n",
    "wsb_df['title'] = wsb_df['title'].str.lower()\n",
    "\n",
    "inv_df['selftext'] = inv_df['selftext'].str.lower()\n",
    "wsb_df['selftext'] = wsb_df['selftext'].str.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our models are case-sensitive, we'll lowercase all words in both the titles and selftext fields within both datasets so that we don't determine that words are different based on case rather than by their intrinsic nature. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation\n",
    "\n",
    "Regex found here [SOURCE](https://stackoverflow.com/questions/44524165/remove-punctuation-in-python-but-keep-emoticons/44524574#44524574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all punctuation \n",
    "\n",
    "inv_df['title'] = inv_df['title'].str.replace(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])','', regex=True)\n",
    "inv_df['selftext'] = inv_df['selftext'].str.replace(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])','', regex=True)\n",
    "\n",
    "wsb_df['title'] = wsb_df['title'].str.replace(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])','', regex=True)\n",
    "wsb_df['selftext'] = wsb_df['selftext'].str.replace(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])','', regex=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a regular expression to strip all punctuation from the posts, but leave emoji's in. Emoji's play an integral role in the recent meme culture that has gripped millenial investors and their impact will be explored at depth below. \n",
    "\n",
    "The punctuation removal is accomplished via regular expression. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove \\n and \\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \\n and \\r from text\n",
    "\n",
    "wsb_df['title'] = wsb_df['title'].str.replace(r'\\n', ' ', regex=True)\n",
    "wsb_df['selftext'] = wsb_df['selftext'].str.replace(r'\\n', ' ', regex=True)\n",
    "\n",
    "# same for investing\n",
    "inv_df['title'] = inv_df['title'].str.replace(r'\\n', ' ', regex=True)\n",
    "inv_df['selftext'] = inv_df['selftext'].str.replace(r'\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wsb_df['title'] = wsb_df['title'].str.replace(r'\\r', ' ', regex=True)\n",
    "wsb_df['selftext'] = wsb_df['selftext'].str.replace(r'\\r', ' ', regex=True)\n",
    "\n",
    "# same for investing\n",
    "inv_df['title'] = inv_df['title'].str.replace(r'\\r', ' ', regex=True)\n",
    "inv_df['selftext'] = inv_df['selftext'].str.replace(r'\\r', ' ', regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all new line characters and carriage returns. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing/Deleted Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title          0\n",
      "selftext     314\n",
      "author         0\n",
      "subreddit      0\n",
      "dtype: int64\n",
      "\n",
      "title          0\n",
      "selftext     170\n",
      "author         0\n",
      "subreddit      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(inv_df.isnull().sum())\n",
    "print('')\n",
    "print(wsb_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5 comments and ill show my fellow degenerates ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>testtging!!!!!!!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>where are the people who bought $fsly puts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>whats the deal with tesla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>how to thrive through end of 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9471</th>\n",
       "      <td>is tesla the new bitcoin bubble</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9474</th>\n",
       "      <td>space x is gonna start paying tesla for exclus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9578</th>\n",
       "      <td>the degenerate wager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9580</th>\n",
       "      <td>what is a great resource for someone who is co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sirtones1411</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9809</th>\n",
       "      <td>robinhood crash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title selftext  \\\n",
       "58    5 comments and ill show my fellow degenerates ...      NaN   \n",
       "396                                    testtging!!!!!!!      NaN   \n",
       "464          where are the people who bought $fsly puts      NaN   \n",
       "857                           whats the deal with tesla      NaN   \n",
       "1016                  how to thrive through end of 2020      NaN   \n",
       "...                                                 ...      ...   \n",
       "9471                    is tesla the new bitcoin bubble      NaN   \n",
       "9474  space x is gonna start paying tesla for exclus...      NaN   \n",
       "9578                               the degenerate wager      NaN   \n",
       "9580  what is a great resource for someone who is co...      NaN   \n",
       "9809                                    robinhood crash      NaN   \n",
       "\n",
       "            author       subreddit  \n",
       "58       [deleted]  wallstreetbets  \n",
       "396      [deleted]  wallstreetbets  \n",
       "464      [deleted]  wallstreetbets  \n",
       "857      [deleted]  wallstreetbets  \n",
       "1016     [deleted]  wallstreetbets  \n",
       "...            ...             ...  \n",
       "9471     [deleted]  wallstreetbets  \n",
       "9474     [deleted]  wallstreetbets  \n",
       "9578     [deleted]  wallstreetbets  \n",
       "9580  sirtones1411  wallstreetbets  \n",
       "9809     [deleted]  wallstreetbets  \n",
       "\n",
       "[170 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>apple during the event hold or sell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>wtf happened here</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>stocks turn negative after trump says white ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>have any stretches helped you with pain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>im up 35 this year but im going back to index ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>citadel securities is cashing in on the retail...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>zerofee trading helps citadel securities cash ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>us home sales tumble to 912year low price grow...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10024</th>\n",
       "      <td>how do you get data for your analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10032</th>\n",
       "      <td>avdv / fndc - which exusa small cap value etf ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>investing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title selftext     author  \\\n",
       "373                  apple during the event hold or sell      NaN  [deleted]   \n",
       "810                                    wtf happened here      NaN  [deleted]   \n",
       "812    stocks turn negative after trump says white ho...      NaN  [deleted]   \n",
       "858              have any stretches helped you with pain      NaN  [deleted]   \n",
       "1137   im up 35 this year but im going back to index ...      NaN  [deleted]   \n",
       "...                                                  ...      ...        ...   \n",
       "10009  citadel securities is cashing in on the retail...      NaN  [deleted]   \n",
       "10010  zerofee trading helps citadel securities cash ...      NaN  [deleted]   \n",
       "10013  us home sales tumble to 912year low price grow...      NaN  [deleted]   \n",
       "10024              how do you get data for your analysis      NaN  [deleted]   \n",
       "10032  avdv / fndc - which exusa small cap value etf ...      NaN  [deleted]   \n",
       "\n",
       "       subreddit  \n",
       "373    investing  \n",
       "810    investing  \n",
       "812    investing  \n",
       "858    investing  \n",
       "1137   investing  \n",
       "...          ...  \n",
       "10009  investing  \n",
       "10010  investing  \n",
       "10013  investing  \n",
       "10024  investing  \n",
       "10032  investing  \n",
       "\n",
       "[314 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# indices of wsb posts with null values\n",
    "wsb_null_idx = wsb_df[wsb_df.isnull().any(axis=1)].index\n",
    "display(wsb_df.iloc[wsb_null_idx])\n",
    "\n",
    "# indices of investing posts with null values\n",
    "inv_null_idx = inv_df[inv_df.isnull().any(axis=1)].index\n",
    "display(inv_df.iloc[inv_null_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r/Investing has 314 posts that are missing selftext and r/WallStreetBets has 170 posts that are missing selftext. The dataframes above demonstrate that is due to the posts being deleted. Since, we have the luxury of working with very popular subreddits and therefore large datasets, we can delete the missing posts and not substantially alter our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9882, 4), (9719, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsb_df.dropna(inplace=True)\n",
    "inv_df.dropna(inplace=True)\n",
    "\n",
    "wsb_df.shape, inv_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# wsb duplicates\n",
    "print(wsb_df.duplicated().sum())\n",
    "wsb_df[wsb_df.duplicated()]\n",
    "\n",
    "# drop duplicates\n",
    "wsb_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "# investing duplicates\n",
    "print(inv_df.duplicated().sum())\n",
    "inv_df[inv_df.duplicated()]\n",
    "\n",
    "\n",
    "# drop duplicates\n",
    "inv_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for any double posts by using the pandas `drop_duplicates` function and remove the double posts. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redact Posts that have \"WSB\" in the subject or body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSB in Title : 246\n",
      "WSB in Body : 670\n"
     ]
    }
   ],
   "source": [
    "# find posts within r/wallstreetbets that have have \"wsb\" in the title or body \n",
    "\n",
    "print('WSB in Title :', wsb_df['title'].str.contains('wsb', case=False).sum())\n",
    "\n",
    "print('WSB in Body :', wsb_df['selftext'].str.contains('wsb', case=False).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSB in Title : 9\n",
      "WSB in Body : 68\n"
     ]
    }
   ],
   "source": [
    "# find posts within r/investing that have have \"wsb\" in the title or body\n",
    "\n",
    "print('WSB in Title :', inv_df['title'].str.contains('wsb', case=False).sum())\n",
    "\n",
    "print('WSB in Body :', inv_df['selftext'].str.contains('wsb', case=False).sum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is important to remove any self-referential terms in the bodies of our datapoints, otherwise the algorithm will not have real predictive power. \n",
    "\n",
    "There are 246 posts that have 'wsb' in the title and 670 posts that have 'wsb' in the subject within r/WallStreetBets. Additionally, there are 8 posts that have 'wsb' within the title in r/Investing and 52 that have 'wsb' in the in the subject. Rather than remove the posts entirely, we'll just replace 'wsb' with whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSB in Title : 0\n",
      "WSB in Body : 0\n"
     ]
    }
   ],
   "source": [
    "# find posts that have \"wsb\" in the body and replace it with whitespace\n",
    "wsb_df['selftext'] = wsb_df['selftext'].str.replace('wsb', ' ', case=False)\n",
    "\n",
    "# find posts that have \"wsb\" in the title and replace it with whitespace\n",
    "wsb_df['title'] = wsb_df['title'].str.replace('wsb', ' ', case=False)\n",
    "\n",
    "# check for redaction\n",
    "print('WSB in Title :', wsb_df['title'].str.contains('wsb', case=False).sum())\n",
    "print('WSB in Body :', wsb_df['selftext'].str.contains('wsb', case=False).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSB in Title : 0\n",
      "WSB in Body : 0\n"
     ]
    }
   ],
   "source": [
    "# find posts that have \"wsb\" in the body and replace it with whitespace\n",
    "inv_df['selftext'] = inv_df['selftext'].str.replace('wsb', ' ', case=False)\n",
    "\n",
    "# find posts that have \"wsb\" in the title and replace it with whitespace\n",
    "inv_df['title'] = inv_df['title'].str.replace('wsb', ' ', case=False)\n",
    "\n",
    "# check for redaction\n",
    "print('WSB in Title :', inv_df['title'].str.contains('wsb', case=False).sum())\n",
    "print('WSB in Body :', inv_df['selftext'].str.contains('wsb', case=False).sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence of URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of r/inv posts with links in selftext:  2157\n",
      "Number of r/inv posts with links in title:  6\n"
     ]
    }
   ],
   "source": [
    "# posts with a link in the selftext\n",
    "print('Number of r/inv posts with links in selftext: ', len(inv_df[inv_df['selftext'].str.contains('http', case=False)]))\n",
    "\n",
    "# posts with a link in the title\n",
    "print('Number of r/inv posts with links in title: ',len(inv_df[inv_df['title'].str.contains('http', case=False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of r/wsb posts with links in selftext:  2314\n",
      "Number of r/wsb posts with links in title:  0\n"
     ]
    }
   ],
   "source": [
    "# posts with a link in the selftext\n",
    "print('Number of r/wsb posts with links in selftext: ', len(wsb_df[wsb_df['selftext'].str.contains('http', case=False)]))\n",
    "\n",
    "# posts with a link in the title\n",
    "print('Number of r/wsb posts with links in title: ',len(wsb_df[wsb_df['title'].str.contains('http', case=False)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both r/WallStreetBets and r/Investing have over 2,000 links contained in their respective selftexts, which accounts for roughly 20% of the respective datasets. This is as expected, as both subreddits center on investing and it stands to reason that the participants would share market news in their posts. Rather than remove these links, we will retain them and simply redact them with the string 'URL'.\n",
    "\n",
    "This task is accomplished below in the SpACy portion of the notebook. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'🚀': 649, '🙌': 8, '🐻': 385, '🐂': 19, '💵': 7, '🍗': 9, '💎': 24}\n",
      "{'🚀': 26, '🙌': 0, '🐻': 0, '🐂': 0, '💵': 6, '🍗': 0, '💎': 0}\n"
     ]
    }
   ],
   "source": [
    "# presence of emojis in wsb df\n",
    "\n",
    "wsb_emojis = ['🚀', '🙌', '🐻', '🐂', '💵', '🍗','💎']\n",
    "\n",
    "def go_brr(df):\n",
    "    emoji_dict = {}\n",
    "    for emoji in wsb_emojis:\n",
    "        emoji_dict[emoji] = len(df['title'].str.findall(emoji).sum()) + len(df['selftext'].str.findall(emoji).sum())\n",
    "    return emoji_dict\n",
    "\n",
    "print(go_brr(wsb_df))\n",
    "print(go_brr(inv_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emoji's are an integral part of WSB culture and frequent posters will routinely express their thoughts and emotions in a combination of the emoji's. \n",
    "\n",
    "For example 💎🙌 represents *Diamond Hands* or the idea that the poster will hold on to their chosen security through periods of great volatility or  drawdown. 🍗 is a meme within a meme, WallStreetBettors often refer to cash as \"tendies\", so the chicken leg is a derivation that flows from Legal Tender -> Tender -> Tendies -> Chicken Tender -> Chicken Tendies -> 🍗. The memeification of investment language and sentiment is so integral to WSB culture that we will have to keep the emoji's in the dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsb\n",
    "wsb_df['selftext'] = wsb_df['selftext'].str.strip() \n",
    "wsb_df['title'] = wsb_df['title'].str.strip() \n",
    "\n",
    "# investing\n",
    "inv_df['selftext'] = inv_df['selftext'].str.strip() \n",
    "inv_df['title'] = inv_df['title'].str.strip() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strip out any additional whitespace that might trip up our classifiers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SpAcy to Lemmatize text, but retain the Emojis and convert hyperlinks to 'URL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "# Create a custom tokenizer that handles emojis and URLs\n",
    "# This is a variation on code I found here (https://stackoverflow.com/questions/51012476/spacy-custom-tokenizer-to-include-only-hyphen-words-as-tokens-using-infix-regex)\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=nlp.tokenizer.infix_finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
    "\n",
    "# Set the custom tokenizer as the tokenizer for the nlp object\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "def lemmatize_text_with_emojis_and_urls(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = []\n",
    "    for token in doc:\n",
    "        if token.like_url:\n",
    "            lemmatized_tokens.append(\"URL\")\n",
    "        elif token.is_punct:\n",
    "            lemmatized_tokens.append(token.text)\n",
    "        else:\n",
    "            lemmatized_tokens.append(token.lemma_.lower())\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply the lemmatize_text_with_emojis_and_urls function to the 'text' column of the DataFrame\n",
    "for df in [wsb_df, inv_df]:\n",
    "    df['selftext_lemmatized'] = df['selftext'].apply(lemmatize_text_with_emojis_and_urls)\n",
    "    df['title_lemmatized'] = df['title'].apply(lemmatize_text_with_emojis_and_urls)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Processing\n",
    "\n",
    "We will use the spaCy library to further process our text. \n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "Tokenization is the process of reducing a body of text into smaller elements, for example turning a sentence into a list of individual words. With these tokenized words, we further process them. \n",
    "\n",
    "**Stemming vs. Lemmatization**\n",
    "\n",
    "We consider the usage of two word processing methods, Stemming and Lemmatization. Stemming removes or \"stems\" characters from a word, without referencing the context of the word. Lemmatization is the act of reducing the word to its base form, which we call a Lemma. An example of the lemmatization vs. stemming process would be the processing the word *Caring*. Lemmatization will return *Care* whereas stemming will return *Car*, which could lead to erroneous interpretations. [SOURCE](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming).\n",
    "\n",
    "Lemmatization is much more computationally expensive than Stemming, however our dataset is not so large as to rule out its practicality so we will use Lemmatization due to its superior accuracy. \n",
    "\n",
    "\n",
    "The custom tokenization and lemmatization process above will not remove emoticons, for the reasons discussed above, and will convert all hyperlinks into the string value `URL`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all ^ symbols\n",
    "\n",
    "for df in [wsb_df, inv_df]:\n",
    "    df['selftext_lemmatized'] = df['selftext_lemmatized'].str.replace('^', ' ', case=False, regex=False)\n",
    "    df['title_lemmatized'] = df['title_lemmatized'].str.replace('^', ' ', case=False, regex=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning process has been effective but has still left some `^` marks where the hyperlinks once were. We will replace them with whitespace. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cleaned data to .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/wallstreetbets\n",
    "wsb_df.to_csv('../data/wsb_df_clean.csv', index=False)\n",
    "\n",
    "# r/investing\n",
    "inv_df.to_csv('../data/inv_df_clean.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save time in the EDA and modelling evolutions, we will save our cleaned data to `.csv` files in our data directory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cda0ac541ab6c535dcb4ffe1de6394d0d0ba460ea4bcec2c3250fd08f595b9ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
